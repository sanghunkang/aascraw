# aascraw

Aascraw is automated-automated web crawler/scrper library. That is, you can collect your desired dataset only by providing some example data which follows schema of your choice. 





We know that there are SOME_KINDS_OF_DATA_I_WANT 
 

### Requirement1: The programme can automatically find a path to the elmement of which contains information of interest.

### Requirement2: The programme can automatically find an axis to iterate over.

How can we mechanically check 
- path can be generated by tree-search fashion
-

a tuple of

There are attributes which we desire to be consistent across iteration, and variant across iteration. For example, we expect that 
 
The primary goal is to find a tuple defined by our arbitary configuration, where we can constrain the range of violation of the definition in a limited error bound.

Ther secondary goal is to find such tup

- container-side attributes
    xpath, tag types, classes, id
- content-side attributes
    semantic type of text

we can expect if container-side attributes are similar and content-side attributes are similar, 


The model:

a set of elements

if a con


  coeff*xpath + coeff*tag_types + coeff*classes + coeff*id
-> this is to ensure that a coe


in an agent - state fashion






The Taskgiver
State1      : That we are looking at some webpage
Action1     : Change the webpage we are looking at
Reward      : USE THE SAME REWARD?

The Taskhandler
State2      : Our consistency and variety of data community
Action2     : Decide to include/exclude new datapoint into the community
Reward      : consistency in structure, variance in contents

# Step 0.
- initial setup
    - define desired schema for collected data. 
    - define storage variables
        - crawled data. maybe I will insert it directly to database
        - xpaths sorted by rank

# Step 1.
- action to move on to the next page
    at the very first execution of the iteration, it is the entry URL
    then, it will be triggered by various actions. e.g. inputting URL with different get query, clicking some button on a page. In any case, it will be sending request to server and receiving response back and working with it.

    The search space for next action will be determined by 
    How taskgiver selects action will be discussed later.

# Step 2 for requirement 1
Rank based - reinforcement algorithm
- collect a data get by xpath(s) with highest rank and compare
    structure remains same and contents are variant: there is a high probability this is what we wanted. We increase the rank of this key
    otherwise: we deem it as uninteresting. We decrease the rank of this key.
    (어떤 element를 찾았는데, 이것이 맞으면 수집을 하고, 맞지 않으면 무시한다.)

    How xpath rank is calculated will be discussed later.

    reinforcement
        objective = minimise structure variance and maximise contents variance
            minimise structure variance
                if addition to the community spoils the structure more than the tolerance level, we reject it.


# Step2 for requirement 2
Add rank to selected actions


# Step 3
After sufficient amount of exploration, we compile state-acion matrix into a procedural codes and execute exploitation.


# how xpath rank is calculated?
A desired property that

A function to find xpath rank is defined accordingly.

- features which contributes to consistency of format
    tag types   ~ 100, consider all. there are
    attributes  ~ 200, for now, concentrate in most common 5
    content types ~ ?

    distance between x2v 
    if within average (+ some buffer) distance -> include to the board
    else reject

- features which contributes to variance of contents
    

# How Taskgiver selects actions?
Unlike evaluating the reward for xpath, which can be calculated by itself, taskgivers reward depends on xpath selection's reward. Analogously speaking, if workers are doing well, that's an evidence that their managers' are doing as well. So we can sensibly reinforce the managers' actions when workers are doing well.

we have to check that our transition policy can bring us 







# Experiments
I just started from a random webpage with arbitrary review.